{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7343e-02ce-411a-b516-4b2a9c2693d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syu2/anaconda3/envs/refact3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:  50%|████████████████████████████████████████████████████████████▌                                                            | 1/2 [00:04<00:04,  4.69s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "#Base model on your local filesystem\n",
    "base_model_dir = \"/home/syu2/.refact/perm-storage/weights/models--codellama--CodeLlama-7b-hf/snapshots/6c284d1468fe6c413cf56183e69b194dcfa27fe6/\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir)\n",
    "\n",
    "#Adaptor directory on your local filesystem\n",
    "adaptor_dir = \"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/checkpoints/iter1050-testloss0.676/\"\n",
    "merged_model = PeftModel.from_pretrained(base_model,adaptor_dir)\n",
    "\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/merged_model_iteration1005\")\n",
    "tokenizer.save_pretrained(\"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/merged_model_iteration1005\", legacy_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013a863c-3703-4a87-afa5-055b7223c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b423a4d791bd4b37916d5a72613330fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Show me an example of sorting algorithm in python?\n",
      "\n",
      "I am trying to implement sorting algorithm in python. I have tried to implement bubble sort, but it is not working.\n",
      "\n",
      "\\begin{code}\n",
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(n-1):\n",
      "        for j in range(n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "    return arr\n",
      "\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "print(bubble_sort(arr))\n",
      "\\end{code}\n",
      "\n",
      "Comment: What do you mean by \"not working\"?\n",
      "\n",
      "Comment: It is not sorting the array.\n",
      "\n",
      "Comment: What is the expected output?\n",
      "\n",
      "Comment: The output should be sorted array.\n",
      "\n",
      "Comment: What is the output you are getting?\n",
      "\n",
      "Comment: The output is same as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment\n"
     ]
    }
   ],
   "source": [
    "# Test the merged model\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "MERGED_MODEL_NAME = \"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/merged_model_iteration1005/\"\n",
    "#BASE_MODEL = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "#config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MERGED_MODEL_NAME, load_in_8bit=True, device_map='cuda:0')\n",
    "\n",
    "\n",
    "final_prompt = 'Show me an example of sorting algorithm in python'\n",
    "\n",
    "try:\n",
    "    input_ids = tokenizer(final_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=512, pad_token_id=128001)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"[make_request_llama3Instruct] Exception: {e}\")\n",
    "   \n",
    "#model = PeftModel.from_pretrained(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0630241-4840-4989-8e77-fdd4ff127ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95db8b2e42c34197863035d264b9c36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Show me an example of sorting algorithm in python?\n",
      "\n",
      "I am trying to write a sorting algorithm in python. I have written the following code:\n",
      "\n",
      "\\begin{code}\n",
      "def bubble_sort(array):\n",
      "    for i in range(len(array)):\n",
      "        for j in range(len(array)-1):\n",
      "            if array[j] > array[j+1]:\n",
      "                array[j], array[j+1] = array[j+1], array[j]\n",
      "    return array\n",
      "\\end{code}\n",
      "\n",
      "I am getting the following error:\n",
      "\n",
      "\\begin{code}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Saurabh\\Desktop\\Python\\bubble_sort.py\", line 11, in <module>\n",
      "    print(bubble_sort([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
      "  File \"C:\\Users\\Saurabh\\Desktop\\Python\\bubble_sort.py\", line 5, in bubble_sort\n",
      "    if array[j] > array[j+1]:\n",
      "IndexError: list index out of range\n",
      "\\end{code}\n",
      "\n",
      "Comment: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Comment: @SaurabhJain: You should accept the answer if it solved your problem.\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Comment: I am getting the following error:\n",
      "\n",
      "Traceback (most recent call\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# save memory\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model_path = \"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/merged_model_iteration1005_refact/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_safetensors=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_8bit=True, use_safetensors=True,  device_map='cuda:0')\n",
    "\n",
    "# # do you have enough vram to run it on gpu?  if so..\n",
    "# model.to(\"cuda:0\")\n",
    "\n",
    "input_string = \"Show me an example of sorting algorithm in python\"\n",
    "\n",
    "# tokenize to ids\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=512, pad_token_id=128001)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa716998-3fb2-452b-a94b-a83dc2d20540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashenv",
   "language": "python",
   "name": "flashenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
